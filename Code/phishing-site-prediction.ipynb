{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Phishing is a method of trying to gather personal information like login credentials or credit card information using deceptive e-mails or  websites.\n",
    "\n",
    "#### Phishing websites are created to dupe unsuspecting users into thinking they are on a legitimate site. The criminals will spend a lot of time making the site seem as credible as possible and many sites will appear almost indistinguishable from the real thing \n",
    "\n",
    "#### So the essence of this code is to determine which sites are used as phishing sites from a given dataset using natural language processing and logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-01-18T01:26:05.551415Z",
     "iopub.status.busy": "2024-01-18T01:26:05.550919Z",
     "iopub.status.idle": "2024-01-18T01:26:05.563922Z",
     "shell.execute_reply": "2024-01-18T01:26:05.562237Z",
     "shell.execute_reply.started": "2024-01-18T01:26:05.551373Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing some useful libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns  \n",
    "import time \n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.tokenize import RegexpTokenizer  \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T01:26:05.566771Z",
     "iopub.status.busy": "2024-01-18T01:26:05.566356Z",
     "iopub.status.idle": "2024-01-18T01:26:06.752225Z",
     "shell.execute_reply": "2024-01-18T01:26:06.751323Z",
     "shell.execute_reply.started": "2024-01-18T01:26:05.566718Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\Olamzkid\\Documents\\Final Year Project\\Dataset\\phishing_site_urls.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T01:26:06.754101Z",
     "iopub.status.busy": "2024-01-18T01:26:06.753542Z",
     "iopub.status.idle": "2024-01-18T01:26:06.785072Z",
     "shell.execute_reply": "2024-01-18T01:26:06.783713Z",
     "shell.execute_reply.started": "2024-01-18T01:26:06.754045Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T01:26:06.787594Z",
     "iopub.status.busy": "2024-01-18T01:26:06.786878Z",
     "iopub.status.idle": "2024-01-18T01:26:06.925226Z",
     "shell.execute_reply": "2024-01-18T01:26:06.924056Z",
     "shell.execute_reply.started": "2024-01-18T01:26:06.787550Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T01:26:06.929818Z",
     "iopub.status.busy": "2024-01-18T01:26:06.929374Z",
     "iopub.status.idle": "2024-01-18T01:26:06.936683Z",
     "shell.execute_reply": "2024-01-18T01:26:06.935368Z",
     "shell.execute_reply.started": "2024-01-18T01:26:06.929774Z"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About data\n",
    "#### The dataset was gotten from kaggle.com which is an open source machine learning research center \n",
    "#### It consists of 549346 rows and 2 columns .The first column consist of links of website and the second column states whether the site is good or bad(phishing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T01:26:06.940759Z",
     "iopub.status.busy": "2024-01-18T01:26:06.940126Z",
     "iopub.status.idle": "2024-01-18T01:26:07.077720Z",
     "shell.execute_reply": "2024-01-18T01:26:07.076335Z",
     "shell.execute_reply.started": "2024-01-18T01:26:06.940703Z"
    }
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T01:26:07.080337Z",
     "iopub.status.busy": "2024-01-18T01:26:07.079671Z",
     "iopub.status.idle": "2024-01-18T01:26:07.960449Z",
     "shell.execute_reply": "2024-01-18T01:26:07.959434Z",
     "shell.execute_reply.started": "2024-01-18T01:26:07.080288Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Label\",data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can Vectorize the URLs.We can gather words from the URLs using Tokenizer\n",
    "#### Tokenization is the process of breaking down a piece of text into smaller units called tokens that makes it easier for the computer to understand and process so in this instance the tokenizer ignores numbers and special characters, extracting only sequences of alphabetic characters\n",
    "### RegexpTokenizer\n",
    "#### we are able to extract the tokens from string by using regular expression with RegexpTokenizer() method.\n",
    "#### [A-Za-z]+ is used to match sequences of one or more alphabetic characters (both uppercase and lowercase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T01:26:07.962171Z",
     "iopub.status.busy": "2024-01-18T01:26:07.961834Z",
     "iopub.status.idle": "2024-01-18T01:26:07.966694Z",
     "shell.execute_reply": "2024-01-18T01:26:07.965654Z",
     "shell.execute_reply.started": "2024-01-18T01:26:07.962139Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'[A-Za-z]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T01:26:07.968704Z",
     "iopub.status.busy": "2024-01-18T01:26:07.968288Z",
     "iopub.status.idle": "2024-01-18T01:26:07.982026Z",
     "shell.execute_reply": "2024-01-18T01:26:07.980851Z",
     "shell.execute_reply.started": "2024-01-18T01:26:07.968671Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.tokenize(df.URL[0]) # this will fetch all the words from the first URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T01:26:07.984026Z",
     "iopub.status.busy": "2024-01-18T01:26:07.983678Z",
     "iopub.status.idle": "2024-01-18T01:26:11.800223Z",
     "shell.execute_reply": "2024-01-18T01:26:11.799145Z",
     "shell.execute_reply.started": "2024-01-18T01:26:07.983994Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenizing all the rows \n",
    "print('Getting words tokenized ...')\n",
    "t0= time.perf_counter()\n",
    "df['text_tokenized'] = df.URL.map(lambda t: tokenizer.tokenize(t))\n",
    "t1 = time.perf_counter() - t0\n",
    "print('Time taken',t1 ,'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T01:26:11.802037Z",
     "iopub.status.busy": "2024-01-18T01:26:11.801667Z",
     "iopub.status.idle": "2024-01-18T01:26:11.924895Z",
     "shell.execute_reply": "2024-01-18T01:26:11.923643Z",
     "shell.execute_reply.started": "2024-01-18T01:26:11.802002Z"
    }
   },
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SnowballStemmer\n",
    "#### Snowball is a small string processing language that gives the root words\n",
    "#### it is essentially an algorithm for reducing words to their base or root form. In other words it shortens words i.e running to run jumping to jump and makes sure that the new words are treated the same as the old ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T01:26:11.927190Z",
     "iopub.status.busy": "2024-01-18T01:26:11.926685Z",
     "iopub.status.idle": "2024-01-18T01:26:11.932499Z",
     "shell.execute_reply": "2024-01-18T01:26:11.931040Z",
     "shell.execute_reply.started": "2024-01-18T01:26:11.927138Z"
    }
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\") # choose a language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T01:26:11.934867Z",
     "iopub.status.busy": "2024-01-18T01:26:11.934383Z",
     "iopub.status.idle": "2024-01-18T01:27:21.537794Z",
     "shell.execute_reply": "2024-01-18T01:27:21.534892Z",
     "shell.execute_reply.started": "2024-01-18T01:26:11.934826Z"
    }
   },
   "outputs": [],
   "source": [
    "# Getting all the stemmed words\n",
    "print('Getting words stemmed ...')\n",
    "t0= time.perf_counter()\n",
    "df['text_stemmed'] = df['text_tokenized'].map(lambda l: [stemmer.stem(word) for word in l])\n",
    "t1= time.perf_counter() - t0\n",
    "print('Time taken',t1 ,'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.538899Z",
     "iopub.status.idle": "2024-01-18T01:27:21.539427Z"
    }
   },
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.540631Z",
     "iopub.status.idle": "2024-01-18T01:27:21.541184Z"
    }
   },
   "outputs": [],
   "source": [
    "# Joining all the stemmmed words.\n",
    "print('Get joiningwords ...')\n",
    "t0= time.perf_counter()\n",
    "df['text_sent'] = df['text_stemmed'].map(lambda l: ' '.join(l))\n",
    "t1= time.perf_counter() - t0\n",
    "print('Time taken',t1 ,'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.542155Z",
     "iopub.status.idle": "2024-01-18T01:27:21.542760Z"
    }
   },
   "outputs": [],
   "source": [
    "bad_sites = df[df.Label == 'bad']\n",
    "good_sites = df[df.Label == 'good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.543768Z",
     "iopub.status.idle": "2024-01-18T01:27:21.544303Z"
    }
   },
   "outputs": [],
   "source": [
    "bad_sites.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.545201Z",
     "iopub.status.idle": "2024-01-18T01:27:21.545856Z"
    }
   },
   "outputs": [],
   "source": [
    "good_sites.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.547000Z",
     "iopub.status.idle": "2024-01-18T01:27:21.547587Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model\n",
    "### CountVectorizer- Convert a collection of text documents to a matrix of token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.548512Z",
     "iopub.status.idle": "2024-01-18T01:27:21.549066Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.550155Z",
     "iopub.status.idle": "2024-01-18T01:27:21.550649Z"
    }
   },
   "outputs": [],
   "source": [
    "feature = cv.fit_transform(df.text_sent) #transform all text which we tokenize and stemed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.551633Z",
     "iopub.status.idle": "2024-01-18T01:27:21.552153Z"
    }
   },
   "outputs": [],
   "source": [
    "feature[:5].toarray() # convert sparse matrix into array to print transformed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.553095Z",
     "iopub.status.idle": "2024-01-18T01:27:21.553582Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.554691Z",
     "iopub.status.idle": "2024-01-18T01:27:21.555288Z"
    }
   },
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(feature, df.Label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression\n",
    "#### Logistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable i.e (binary classification task) (yes, success, etc.) or 0 (no, failure, etc.)\n",
    "#### Logistic regression is a statistical model commonly used for , where the outcome is one of two possible classes (e.g., spam or not spam, phishing or not phishing).\n",
    "\n",
    "When applied to text classification tasks:\n",
    "\n",
    "Tokenization: This process involves breaking down the text into individual units, such as words or tokens.\n",
    "\n",
    "Vectorization: After tokenization, the text data is converted into numerical format (vectors) \n",
    "\n",
    "Logistic Regression: The logistic regression model then takes these vectors as input features. It learns the relationship between the features (vectorized words) and the binary outcome (e.g., phishing or not phishing).\n",
    "\n",
    "Thats why im using logistic regression in this particular code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.556352Z",
     "iopub.status.idle": "2024-01-18T01:27:21.556926Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.560459Z",
     "iopub.status.idle": "2024-01-18T01:27:21.561066Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.562269Z",
     "iopub.status.idle": "2024-01-18T01:27:21.562842Z"
    }
   },
   "outputs": [],
   "source": [
    "lr.score(testX,testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression provide 96% accuracy,Now we will store the score in the dictionary so that we can find which model performs the best.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.563987Z",
     "iopub.status.idle": "2024-01-18T01:27:21.564533Z"
    }
   },
   "outputs": [],
   "source": [
    "Scores_ml = {}\n",
    "Scores_ml['Logistic Regression'] = np.round(lr.score(testX,testY),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.566118Z",
     "iopub.status.idle": "2024-01-18T01:27:21.566705Z"
    }
   },
   "outputs": [],
   "source": [
    "# creating confusing matrix\n",
    "print('Training Accuracy :',lr.score(trainX,trainY))\n",
    "print('Testing Accuracy :',lr.score(testX,testY))\n",
    "con_mat = pd.DataFrame(confusion_matrix(lr.predict(testX), testY),\n",
    "            columns = ['Predicted:Bad', 'Predicted:Good'],\n",
    "            index = ['Actual:Bad', 'Actual:Good'])\n",
    "\n",
    "\n",
    "print('\\nCLASSIFICATION REPORT\\n')\n",
    "print(classification_report(lr.predict(testX), testY,\n",
    "                            target_names =['Bad','Good']))\n",
    "\n",
    "print('\\nCONFUSION MATRIX')\n",
    "plt.figure(figsize= (6,4))\n",
    "sns.heatmap(con_mat, annot = True,fmt='d',cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultinomialNB\n",
    "\n",
    "#### The multinomial Naive Bayes classifier is for text classification tasks, particularly when dealing with features that represent counts or frequencies of tokens which is exactly what ive been doing since \n",
    "\n",
    "#### The naive bayes classifier calculates the probability of the document belonging to each class using the features (word frequencies). It then predicts the class with the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.568410Z",
     "iopub.status.idle": "2024-01-18T01:27:21.569017Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.570108Z",
     "iopub.status.idle": "2024-01-18T01:27:21.570637Z"
    }
   },
   "outputs": [],
   "source": [
    "# create mnb object\n",
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.571716Z",
     "iopub.status.idle": "2024-01-18T01:27:21.572585Z"
    }
   },
   "outputs": [],
   "source": [
    "mnb.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.574028Z",
     "iopub.status.idle": "2024-01-18T01:27:21.574572Z"
    }
   },
   "outputs": [],
   "source": [
    "mnb.score(testX,testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultinomialNB provide 95% accuracy,so we can store the score in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.575580Z",
     "iopub.status.idle": "2024-01-18T01:27:21.576175Z"
    }
   },
   "outputs": [],
   "source": [
    "Scores_ml['MultinomialNB'] = np.round(mnb.score(testX,testY),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.577415Z",
     "iopub.status.idle": "2024-01-18T01:27:21.577944Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Training Accuracy :',mnb.score(trainX,trainY))\n",
    "print('Testing Accuracy :',mnb.score(testX,testY))\n",
    "con_mat = pd.DataFrame(confusion_matrix(mnb.predict(testX), testY),\n",
    "            columns = ['Predicted:Bad', 'Predicted:Good'],\n",
    "            index = ['Actual:Bad', 'Actual:Good'])\n",
    "\n",
    "\n",
    "print('\\nCLASSIFICATION REPORT\\n')\n",
    "print(classification_report(mnb.predict(testX), testY,\n",
    "                            target_names =['Bad','Good']))\n",
    "\n",
    "print('\\nCONFUSION MATRIX')\n",
    "plt.figure(figsize= (6,4))\n",
    "sns.heatmap(con_mat, annot = True,fmt='d',cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So, Logistic Regression is the best fit for this model, Now lets make sklearn pipeline using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.580656Z",
     "iopub.status.idle": "2024-01-18T01:27:21.581193Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline_ls = make_pipeline(CountVectorizer(tokenizer = RegexpTokenizer(r'[A-Za-z]+').tokenize,stop_words='english'), LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the code above works by :\n",
    "\n",
    "#### It first tokenizes the input text using the specified regular expression tokenizer (RegexpTokenizer(r'[A-Za-z]+').tokenize).\n",
    "#### It removes English stopwords.\n",
    "#### It then converts the text data into a matrix of token counts (using CountVectorizer).\n",
    "#### Finally, it applies logistic regression (LogisticRegression()) to classify the text based on the features generated by CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.582242Z",
     "iopub.status.idle": "2024-01-18T01:27:21.582754Z"
    }
   },
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(df.URL, df.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.583795Z",
     "iopub.status.idle": "2024-01-18T01:27:21.584345Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline_ls.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.585388Z",
     "iopub.status.idle": "2024-01-18T01:27:21.585899Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline_ls.score(testX,testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.586938Z",
     "iopub.status.idle": "2024-01-18T01:27:21.587443Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Training Accuracy :',pipeline_ls.score(trainX,trainY))\n",
    "print('Testing Accuracy :',pipeline_ls.score(testX,testY))\n",
    "con_mat = pd.DataFrame(confusion_matrix(pipeline_ls.predict(testX), testY),\n",
    "            columns = ['Predicted:Bad', 'Predicted:Good'],\n",
    "            index = ['Actual:Bad', 'Actual:Good'])\n",
    "\n",
    "\n",
    "print('\\nCLASSIFICATION REPORT\\n')\n",
    "print(classification_report(pipeline_ls.predict(testX), testY,\n",
    "                            target_names =['Bad','Good']))\n",
    "\n",
    "print('\\nCONFUSION MATRIX')\n",
    "plt.figure(figsize= (6,4))\n",
    "sns.heatmap(con_mat, annot = True,fmt='d',cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we dump the model in pickle format\n",
    "#### pickle files are a convenient way to store and retrieve Python objects, including machine learning models and data, which helps streamline the development and deployment of machine learning applications\n",
    "\n",
    "we can use it to store models for future use without having to retain the mode every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.588405Z",
     "iopub.status.idle": "2024-01-18T01:27:21.588949Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('phishingApp.pkl', 'wb') as model_file:\n",
    "    pickle.dump(pipeline_ls, model_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.589879Z",
     "iopub.status.idle": "2024-01-18T01:27:21.590382Z"
    }
   },
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open('phishingApp.pkl', 'rb'))\n",
    "result = loaded_model.score(testX,testY)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 791543,
     "sourceId": 1359146,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30066,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
