{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Phishing is a method of trying to gather personal information like login credentials or credit card information using deceptive e-mails or  websites.\n",
    "\n",
    "#### Phishing websites are created to dupe unsuspecting users into thinking they are on a legitimate site. The criminals will spend a lot of time making the site seem as credible as possible and many sites will appear almost indistinguishable from the real thing \n",
    "\n",
    "#### So the essence of this code is to determine which sites are used as phishing sites from a given dataset using natural language processing and logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-01-18T01:26:05.551415Z",
     "iopub.status.busy": "2024-01-18T01:26:05.550919Z",
     "iopub.status.idle": "2024-01-18T01:26:05.563922Z",
     "shell.execute_reply": "2024-01-18T01:26:05.562237Z",
     "shell.execute_reply.started": "2024-01-18T01:26:05.551373Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing some useful libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns  \n",
    "import time \n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.tokenize import RegexpTokenizer  \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T01:26:05.566771Z",
     "iopub.status.busy": "2024-01-18T01:26:05.566356Z",
     "iopub.status.idle": "2024-01-18T01:26:06.752225Z",
     "shell.execute_reply": "2024-01-18T01:26:06.751323Z",
     "shell.execute_reply.started": "2024-01-18T01:26:05.566718Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\Olamzkid\\Documents\\Final Year Project\\Dataset\\phishing_site_urls Combined.csv\")\n",
    "\n",
    "# Drop rows with NaN values in 'URL' or 'Label'\n",
    "df = df.dropna(subset=['URL', 'Label'])\n",
    "\n",
    "# Convert all URLs to strings to avoid TypeError\n",
    "df['URL'] = df['URL'].astype(str)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = RegexpTokenizer(r'[A-Za-z]+')\n",
    "df['text_tokenized'] = df.URL.map(lambda t: tokenizer.tokenize(t) if isinstance(t, str) and t.strip() else [])\n",
    "\n",
    "# Stemming\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "df['text_stemmed'] = df['text_tokenized'].map(lambda l: [stemmer.stem(word) for word in l])\n",
    "\n",
    "# Joining Stems\n",
    "df['text_sent'] = df['text_stemmed'].map(lambda l: ' '.join(l))\n",
    "\n",
    "# Drop any rows where 'text_sent' is empty after preprocessing\n",
    "df = df[df['text_sent'].str.strip() != '']\n",
    "\n",
    "# Data summary and visualization\n",
    "df.head()\n",
    "df.info()\n",
    "df.shape\n",
    "df.isnull().sum()\n",
    "\n",
    "# Visualizing the distribution of labels\n",
    "sns.countplot(x=\"Label\", data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T01:26:06.754101Z",
     "iopub.status.busy": "2024-01-18T01:26:06.753542Z",
     "iopub.status.idle": "2024-01-18T01:26:06.785072Z",
     "shell.execute_reply": "2024-01-18T01:26:06.783713Z",
     "shell.execute_reply.started": "2024-01-18T01:26:06.754045Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T01:26:06.787594Z",
     "iopub.status.busy": "2024-01-18T01:26:06.786878Z",
     "iopub.status.idle": "2024-01-18T01:26:06.925226Z",
     "shell.execute_reply": "2024-01-18T01:26:06.924056Z",
     "shell.execute_reply.started": "2024-01-18T01:26:06.787550Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T01:26:06.929818Z",
     "iopub.status.busy": "2024-01-18T01:26:06.929374Z",
     "iopub.status.idle": "2024-01-18T01:26:06.936683Z",
     "shell.execute_reply": "2024-01-18T01:26:06.935368Z",
     "shell.execute_reply.started": "2024-01-18T01:26:06.929774Z"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About data\n",
    "#### The dataset was gotten from kaggle.com which is an open source machine learning research center & Phishtank which is a verified source for cybersecurity concerning phishing\n",
    "#### the 2 data sets combined consists of 629325 rows and 2 columns .The first column consist of links of website and the second column states whether the site is good or bad(phishing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T01:26:06.940759Z",
     "iopub.status.busy": "2024-01-18T01:26:06.940126Z",
     "iopub.status.idle": "2024-01-18T01:26:07.077720Z",
     "shell.execute_reply": "2024-01-18T01:26:07.076335Z",
     "shell.execute_reply.started": "2024-01-18T01:26:06.940703Z"
    }
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T01:26:07.080337Z",
     "iopub.status.busy": "2024-01-18T01:26:07.079671Z",
     "iopub.status.idle": "2024-01-18T01:26:07.960449Z",
     "shell.execute_reply": "2024-01-18T01:26:07.959434Z",
     "shell.execute_reply.started": "2024-01-18T01:26:07.080288Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Label\",data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can Vectorize the URLs.We can gather words from the URLs using Tokenizer\n",
    "#### Tokenization is the process of breaking down a piece of text into smaller units called tokens that makes it easier for the computer to understand and process so in this instance the tokenizer ignores numbers and special characters, extracting only sequences of alphabetic characters\n",
    "### RegexpTokenizer\n",
    "#### we are able to extract the tokens from string by using regular expression with RegexpTokenizer() method.\n",
    "#### [A-Za-z]+ is used to match sequences of one or more alphabetic characters (both uppercase and lowercase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T01:26:07.962171Z",
     "iopub.status.busy": "2024-01-18T01:26:07.961834Z",
     "iopub.status.idle": "2024-01-18T01:26:07.966694Z",
     "shell.execute_reply": "2024-01-18T01:26:07.965654Z",
     "shell.execute_reply.started": "2024-01-18T01:26:07.962139Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Tokenization\n",
    "tokenizer = RegexpTokenizer(r'[A-Za-z]+')\n",
    "\n",
    "# Apply tokenization to the URLs\n",
    "df['text_tokenized'] = df.URL.map(lambda t: tokenizer.tokenize(t) if isinstance(t, str) and t.strip() else [])\n",
    "\n",
    "# Stemming\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "df['text_stemmed'] = df['text_tokenized'].map(lambda l: [stemmer.stem(word) for word in l])\n",
    "\n",
    "# Joining Stems\n",
    "df['text_sent'] = df['text_stemmed'].map(lambda l: ' '.join(l))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T01:26:07.968704Z",
     "iopub.status.busy": "2024-01-18T01:26:07.968288Z",
     "iopub.status.idle": "2024-01-18T01:26:07.982026Z",
     "shell.execute_reply": "2024-01-18T01:26:07.980851Z",
     "shell.execute_reply.started": "2024-01-18T01:26:07.968671Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.tokenize(df.URL[0]) # this will fetch all the words from the first URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T01:26:07.984026Z",
     "iopub.status.busy": "2024-01-18T01:26:07.983678Z",
     "iopub.status.idle": "2024-01-18T01:26:11.800223Z",
     "shell.execute_reply": "2024-01-18T01:26:11.799145Z",
     "shell.execute_reply.started": "2024-01-18T01:26:07.983994Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenizing all the rows \n",
    "print('Getting words tokenized ...')\n",
    "t0= time.perf_counter()\n",
    "df['text_tokenized'] = df.URL.map(lambda t: tokenizer.tokenize(t))\n",
    "t1 = time.perf_counter() - t0\n",
    "print('Time taken',t1 ,'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T01:26:11.802037Z",
     "iopub.status.busy": "2024-01-18T01:26:11.801667Z",
     "iopub.status.idle": "2024-01-18T01:26:11.924895Z",
     "shell.execute_reply": "2024-01-18T01:26:11.923643Z",
     "shell.execute_reply.started": "2024-01-18T01:26:11.802002Z"
    }
   },
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SnowballStemmer\n",
    "#### Snowball is a small string processing language that gives the root words\n",
    "#### it is essentially an algorithm for reducing words to their base or root form. In other words it shortens words i.e running to run jumping to jump and makes sure that the new words are treated the same as the old ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T01:26:11.927190Z",
     "iopub.status.busy": "2024-01-18T01:26:11.926685Z",
     "iopub.status.idle": "2024-01-18T01:26:11.932499Z",
     "shell.execute_reply": "2024-01-18T01:26:11.931040Z",
     "shell.execute_reply.started": "2024-01-18T01:26:11.927138Z"
    }
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\") # choose a language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T01:26:11.934867Z",
     "iopub.status.busy": "2024-01-18T01:26:11.934383Z",
     "iopub.status.idle": "2024-01-18T01:27:21.537794Z",
     "shell.execute_reply": "2024-01-18T01:27:21.534892Z",
     "shell.execute_reply.started": "2024-01-18T01:26:11.934826Z"
    }
   },
   "outputs": [],
   "source": [
    "# Getting all the stemmed words\n",
    "print('Getting words stemmed ...')\n",
    "t0= time.perf_counter()\n",
    "df['text_stemmed'] = df['text_tokenized'].map(lambda l: [stemmer.stem(word) for word in l])\n",
    "t1= time.perf_counter() - t0\n",
    "print('Time taken',t1 ,'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.538899Z",
     "iopub.status.idle": "2024-01-18T01:27:21.539427Z"
    }
   },
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.540631Z",
     "iopub.status.idle": "2024-01-18T01:27:21.541184Z"
    }
   },
   "outputs": [],
   "source": [
    "# Joining all the stemmmed words.\n",
    "print('Get joiningwords ...')\n",
    "t0= time.perf_counter()\n",
    "df['text_sent'] = df['text_stemmed'].map(lambda l: ' '.join(l))\n",
    "t1= time.perf_counter() - t0\n",
    "print('Time taken',t1 ,'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.542155Z",
     "iopub.status.idle": "2024-01-18T01:27:21.542760Z"
    }
   },
   "outputs": [],
   "source": [
    "bad_sites = df[df.Label == 'bad']\n",
    "good_sites = df[df.Label == 'good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.543768Z",
     "iopub.status.idle": "2024-01-18T01:27:21.544303Z"
    }
   },
   "outputs": [],
   "source": [
    "bad_sites.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.545201Z",
     "iopub.status.idle": "2024-01-18T01:27:21.545856Z"
    }
   },
   "outputs": [],
   "source": [
    "good_sites.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.547000Z",
     "iopub.status.idle": "2024-01-18T01:27:21.547587Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model\n",
    "### CountVectorizer- Convert a collection of text documents to a matrix of token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.548512Z",
     "iopub.status.idle": "2024-01-18T01:27:21.549066Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.550155Z",
     "iopub.status.idle": "2024-01-18T01:27:21.550649Z"
    }
   },
   "outputs": [],
   "source": [
    "feature = cv.fit_transform(df.text_sent) #transform all text which we tokenize and stemed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.551633Z",
     "iopub.status.idle": "2024-01-18T01:27:21.552153Z"
    }
   },
   "outputs": [],
   "source": [
    "feature[:5].toarray() # convert sparse matrix into array to print transformed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.553095Z",
     "iopub.status.idle": "2024-01-18T01:27:21.553582Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.554691Z",
     "iopub.status.idle": "2024-01-18T01:27:21.555288Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Feature Extraction with the first vectorizer (used only for exploration)\n",
    "cv = CountVectorizer()\n",
    "feature = cv.fit_transform(df.text_sent)\n",
    "\n",
    "# Train-Test Split\n",
    "trainX, testX, trainY, testY = train_test_split(feature, df.Label, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression (Initial Exploration)\n",
    "lr = LogisticRegression()\n",
    "lr.fit(trainX, trainY)\n",
    "print('Training Accuracy :', lr.score(trainX, trainY))\n",
    "print('Testing Accuracy :', lr.score(testX, testY))\n",
    "\n",
    "# Multinomial Naive Bayes (Initial Exploration)\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(trainX, trainY)\n",
    "print('Training Accuracy :', mnb.score(trainX, trainY))\n",
    "print('Testing Accuracy :', mnb.score(testX, testY))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression\n",
    "#### Logistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable i.e (binary classification task) (yes, success, etc.) or 0 (no, failure, etc.)\n",
    "#### Logistic regression is a statistical model commonly used for , where the outcome is one of two possible classes (e.g., spam or not spam, phishing or not phishing).\n",
    "\n",
    "When applied to text classification tasks:\n",
    "\n",
    "Tokenization: This process involves breaking down the text into individual units, such as words or tokens.\n",
    "\n",
    "Vectorization: After tokenization, the text data is converted into numerical format (vectors) \n",
    "\n",
    "Logistic Regression: The logistic regression model then takes these vectors as input features. It learns the relationship between the features (vectorized words) and the binary outcome (e.g., phishing or not phishing).\n",
    "\n",
    "Thats why im using logistic regression in this particular code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.556352Z",
     "iopub.status.idle": "2024-01-18T01:27:21.556926Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.560459Z",
     "iopub.status.idle": "2024-01-18T01:27:21.561066Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.562269Z",
     "iopub.status.idle": "2024-01-18T01:27:21.562842Z"
    }
   },
   "outputs": [],
   "source": [
    "lr.score(testX,testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression provide 96% accuracy,Now we will store the score in the dictionary so that we can find which model performs the best.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.563987Z",
     "iopub.status.idle": "2024-01-18T01:27:21.564533Z"
    }
   },
   "outputs": [],
   "source": [
    "Scores_ml = {}\n",
    "Scores_ml['Logistic Regression'] = np.round(lr.score(testX,testY),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.566118Z",
     "iopub.status.idle": "2024-01-18T01:27:21.566705Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating and evaluating the confusion matrix\n",
    "print('Training Accuracy:', lr.score(trainX, trainY))\n",
    "print('Testing Accuracy:', lr.score(testX, testY))\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "con_mat = confusion_matrix(testY, lr.predict(testX))\n",
    "\n",
    "# Dynamically generate labels based on the unique classes in testY\n",
    "labels = sorted(set(testY))\n",
    "predicted_labels = [f'Predicted:{cls}' for cls in labels]\n",
    "actual_labels = [f'Actual:{cls}' for cls in labels]\n",
    "\n",
    "# Convert the confusion matrix to a DataFrame\n",
    "con_mat_df = pd.DataFrame(con_mat, columns=predicted_labels, index=actual_labels)\n",
    "\n",
    "print('\\nCLASSIFICATION REPORT\\n')\n",
    "print(classification_report(testY, lr.predict(testX), target_names=labels))\n",
    "\n",
    "print('\\nCONFUSION MATRIX')\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(con_mat_df, annot=True, fmt='d', cmap=\"YlGnBu\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultinomialNB\n",
    "\n",
    "#### The multinomial Naive Bayes classifier is for text classification tasks, particularly when dealing with features that represent counts or frequencies of tokens which is exactly what ive been doing since \n",
    "\n",
    "#### The naive bayes classifier calculates the probability of the document belonging to each class using the features (word frequencies). It then predicts the class with the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.568410Z",
     "iopub.status.idle": "2024-01-18T01:27:21.569017Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.570108Z",
     "iopub.status.idle": "2024-01-18T01:27:21.570637Z"
    }
   },
   "outputs": [],
   "source": [
    "# create mnb object\n",
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.571716Z",
     "iopub.status.idle": "2024-01-18T01:27:21.572585Z"
    }
   },
   "outputs": [],
   "source": [
    "mnb.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.574028Z",
     "iopub.status.idle": "2024-01-18T01:27:21.574572Z"
    }
   },
   "outputs": [],
   "source": [
    "mnb.score(testX,testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultinomialNB provide 95% accuracy,so we can store the score in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.575580Z",
     "iopub.status.idle": "2024-01-18T01:27:21.576175Z"
    }
   },
   "outputs": [],
   "source": [
    "Scores_ml['MultinomialNB'] = np.round(mnb.score(testX,testY),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.577415Z",
     "iopub.status.idle": "2024-01-18T01:27:21.577944Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the Multinomial Naive Bayes model\n",
    "print('Training Accuracy:', mnb.score(trainX, trainY))\n",
    "print('Testing Accuracy:', mnb.score(testX, testY))\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "y_true = testY\n",
    "y_pred = mnb.predict(testX)\n",
    "con_mat = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Get the unique classes from both true labels and predictions\n",
    "labels = sorted(set(y_true) | set(y_pred))  # Union of both sets\n",
    "\n",
    "# Generate labels for the DataFrame\n",
    "predicted_labels = [f'Predicted:{cls}' for cls in labels]\n",
    "actual_labels = [f'Actual:{cls}' for cls in labels]\n",
    "\n",
    "# Convert the confusion matrix to a DataFrame\n",
    "con_mat_df = pd.DataFrame(con_mat, columns=predicted_labels, index=actual_labels)\n",
    "\n",
    "print('\\nCLASSIFICATION REPORT\\n')\n",
    "print(classification_report(y_true, y_pred, target_names=labels))\n",
    "\n",
    "print('\\nCONFUSION MATRIX')\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(con_mat_df, annot=True, fmt='d', cmap=\"YlGnBu\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So, Logistic Regression is the best fit for this model, Now lets make sklearn pipeline using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.580656Z",
     "iopub.status.idle": "2024-01-18T01:27:21.581193Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Tokenization\n",
    "tokenizer = RegexpTokenizer(r'[A-Za-z]+')\n",
    "\n",
    "# Apply tokenization to the URLs\n",
    "df['text_tokenized'] = df.URL.map(lambda t: tokenizer.tokenize(t) if isinstance(t, str) and t.strip() else [])\n",
    "\n",
    "# Stemming\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "df['text_stemmed'] = df['text_tokenized'].map(lambda l: [stemmer.stem(word) for word in l])\n",
    "\n",
    "# Joining Stems\n",
    "df['text_sent'] = df['text_stemmed'].map(lambda l: ' '.join(l))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the code above works by :\n",
    "\n",
    "#### It first tokenizes the input text using the specified regular expression tokenizer (RegexpTokenizer(r'[A-Za-z]+').tokenize).\n",
    "#### It removes English stopwords.\n",
    "#### It then converts the text data into a matrix of token counts (using CountVectorizer).\n",
    "#### Finally, it applies logistic regression (LogisticRegression()) to classify the text based on the features generated by CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.582242Z",
     "iopub.status.idle": "2024-01-18T01:27:21.582754Z"
    }
   },
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(df.URL, df.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.583795Z",
     "iopub.status.idle": "2024-01-18T01:27:21.584345Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Define the pipeline with tokenization and Logistic Regression\n",
    "pipeline_ls = make_pipeline(\n",
    "    CountVectorizer(tokenizer=RegexpTokenizer(r'[A-Za-z]+').tokenize, stop_words='english'),\n",
    "    LogisticRegression()\n",
    ")\n",
    "\n",
    "# Train-test split using df.URL and df.Label\n",
    "trainX, testX, trainY, testY = train_test_split(df['URL'], df['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline_ls.fit(trainX, trainY)\n",
    "\n",
    "# Evaluate the pipeline\n",
    "print('Training Accuracy:', pipeline_ls.score(trainX, trainY))\n",
    "print('Testing Accuracy:', pipeline_ls.score(testX, testY))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.586938Z",
     "iopub.status.idle": "2024-01-18T01:27:21.587443Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate training and testing accuracy\n",
    "print('Training Accuracy :', pipeline_ls.score(trainX, trainY))\n",
    "print('Testing Accuracy :', pipeline_ls.score(testX, testY))\n",
    "\n",
    "# Predict the labels for the test set\n",
    "predictions = pipeline_ls.predict(testX)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "con_mat = confusion_matrix(testY, predictions)\n",
    "\n",
    "# Determine the number of classes and generate class labels\n",
    "num_classes = con_mat.shape[0]\n",
    "class_labels = sorted(set(testY))  # Extract unique classes from true labels\n",
    "\n",
    "# Verify that class labels match the confusion matrix shape\n",
    "if num_classes != len(class_labels):\n",
    "    raise ValueError(f\"Expected {len(class_labels)} classes but got {num_classes}.\")\n",
    "\n",
    "# Create DataFrame for the confusion matrix\n",
    "con_mat_df = pd.DataFrame(con_mat,\n",
    "                          columns=[f'Predicted:{label}' for label in class_labels],\n",
    "                          index=[f'Actual:{label}' for label in class_labels])\n",
    "\n",
    "# Print classification report\n",
    "print('\\nCLASSIFICATION REPORT\\n')\n",
    "print(classification_report(testY, predictions, target_names=class_labels))\n",
    "\n",
    "# Print confusion matrix\n",
    "print('\\nCONFUSION MATRIX')\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(con_mat_df, annot=True, fmt='d', cmap=\"YlGnBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we dump the model in pickle format\n",
    "#### pickle files are a convenient way to store and retrieve Python objects, including machine learning models and data, which helps streamline the development and deployment of machine learning applications\n",
    "\n",
    "we can use it to store models for future use without having to retain the mode every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.588405Z",
     "iopub.status.idle": "2024-01-18T01:27:21.588949Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('phishingApp.pkl', 'wb') as model_file:\n",
    "    pickle.dump(pipeline_ls, model_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-18T01:27:21.589879Z",
     "iopub.status.idle": "2024-01-18T01:27:21.590382Z"
    }
   },
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open('phishingApp.pkl', 'rb'))\n",
    "result = loaded_model.score(testX,testY)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 791543,
     "sourceId": 1359146,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30066,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
